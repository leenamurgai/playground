{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collective Operations\n",
    "\n",
    "### Sources\n",
    "\n",
    "- [Wikipedia: Collective operation](https://en.wikipedia.org/wiki/Collective_operation)\n",
    "- [Wikipedia: Broadcast parallel pattern](https://en.wikipedia.org/wiki/Broadcast_(parallel_pattern))\n",
    "- [MPI Tutorial](https://mpitutorial.com/tutorials/mpi-reduce-and-allreduce)\n",
    "- [Python MPI library](https://materials.jeremybejarano.com/MPIwithPython/collectiveCom.html)\n",
    "- [Python MPI tutorial](https://nyu-cds.github.io/python-mpi/)\n",
    "\n",
    "### Notation\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p          & = \\textrm{Number of processing units}                                             && \\\\\n",
    "\\mathbf{p} & = (p_0, p_1,...,p_{p-1}) & p_i & = \\textrm{Processing unit / node }i                 \\\\\n",
    "\\mathbf{n} & = (n_0, n_1,...,n_{p-1}) & n_i & = \\textrm{Input message size for processing unit }i \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "- In cases where we have initial messages on more than one node we assume that all local messages are of the same size.\n",
    "- If we do not have an equal distribution, i.e. node $p_i$ has a message of size $n_i$, we get an upper bound for the runtime by setting $n=\\max(n_{0},n_{1},\\dots ,n_{p-1})$.\n",
    "- A distributed memory model is assumed where each processing unit has it's own associated memory.\n",
    "\n",
    "## 1. Broadcast\n",
    "\n",
    "- The broadcast pattern is used to distribute data from one processing unit to all processing units\n",
    "- Broadcast can be interpreted as an inverse version of the reduce pattern\n",
    "- Initially only root $r$ with $id=0$ stores message $m$.\n",
    "- During broadcast $m$ is sent to the remaining processing units, so that eventually $m$ is available to all processing units.\n",
    "- Since an implementation by means of a sequential for-loop with $p-1$ iterations becomes a bottleneck, divide-and-conquer approaches are common.\n",
    "\n",
    "![fig](figures/Broadcast.png)\n",
    "By RenderFlamingo - Own work, CC BY-SA 4.0, [curid=86786881](https://commons.wikimedia.org/w/index.php?curid=86786881).\n",
    "\n",
    "### 1.1. Binomial Tree Broadcast\n",
    "\n",
    "- $p$ has to be a power of two.\n",
    "- When a processing unit is responsible for sending $m$ to processing units $[i,j]$,\n",
    "- it sends $m$ to processing unit $(i+j)/2$ and delegates responsibility for the processing units $\\left\\lceil (i+j)/2\\right\\rceil ..\\left\\lceil (i+j)-1\\right\\rceil$ to it,\n",
    "- while its own responsibility is cut down to $i..\\left\\lceil (i+j)/2\\right\\rceil -1$.\n",
    "- Binomial trees have a problem with long messages $m$. The receiving unit of $m$ can only propagate the message to other units, after it received the whole message. In the meantime, the communication network is not utilized.\n",
    "- Broadcast on balanced binary tree is possible in $\\mathrm{O}(\\alpha \\log p)$, where $\\alpha$ is the latency.\n",
    "\n",
    "![fig](figures/Binomial_Tree_Broadcast.gif)\n",
    "By 13hannes1 - Own work, CC BY-SA 4.0, [curid=77379340](https://commons.wikimedia.org/w/index.php?curid=77379340)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All library imports here\n",
    "\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n    = 4\n",
    "msg  = [1,1]\n",
    "k    = len(msg)\n",
    "\n",
    "p    = [[0]*k for i in range(n)]\n",
    "p[0] = msg\n",
    "\n",
    "print('Binary broadcast with simple recursive algorithm')\n",
    "print()\n",
    "print('i j s')\n",
    "print('- -', sum([sum(p[m]) for m in range(n)]), '\\t',p)\n",
    "\n",
    "def broadcast(i, j, msg):\n",
    "    if j-i == 1:\n",
    "        p[j] = msg\n",
    "        print(i,j, sum([sum(p[m]) for m in range(n)]), '\\t',p)\n",
    "        return\n",
    "    else:\n",
    "        newi = int((i+j)/2)+1\n",
    "        p[newi] = msg\n",
    "        print(i,j, sum([sum(p[m]) for m in range(n)]), '\\t',p)\n",
    "        broadcast(i, newi-1, msg)\n",
    "        broadcast(newi, j, msg)\n",
    "\n",
    "broadcast(0, n-1, msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Linear Broadcast\n",
    "\n",
    "- The message is $k$ pieces long and you can only copy over one piece at a time\n",
    "- Pipelined broadcast on balanced binary tree is possible in $\\mathrm{O}(\\alpha (p+k))$.\n",
    "\n",
    "![fig](figures/Pipeline_Broadcast.gif)\n",
    "\n",
    "By 13hannes1 - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=77379385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear pipeline broadcast with simple recursive algorithm\n",
      "\n",
      "n k s\n",
      "0 0 2 \t [[1, 1], [0, 0], [0, 0], [0, 0]]\n",
      "0 1 3 \t [[1, 1], [1, 0], [0, 0], [0, 0]]\n",
      "1 0 4 \t [[1, 1], [1, 1], [0, 0], [0, 0]]\n",
      "1 1 5 \t [[1, 1], [1, 1], [1, 0], [0, 0]]\n",
      "2 0 6 \t [[1, 1], [1, 1], [1, 1], [0, 0]]\n",
      "2 1 7 \t [[1, 1], [1, 1], [1, 1], [1, 0]]\n",
      "3 0 8 \t [[1, 1], [1, 1], [1, 1], [1, 1]]\n"
     ]
    }
   ],
   "source": [
    "p    = [[0]*k for i in range(n)]\n",
    "p[0] = msg\n",
    "\n",
    "print('Linear pipeline broadcast with simple recursive algorithm')\n",
    "print()\n",
    "print('n', 'k', 's')\n",
    "def linear_broadcast(node, mi, p):\n",
    "    n, k = len(p), len(p[0])\n",
    "    if mi==k:\n",
    "        return\n",
    "    if node==n and mi==k:\n",
    "        return\n",
    "    print(node, mi, sum([sum(p[i]) for i in range(n)]), '\\t', p)\n",
    "    if node<n-1:\n",
    "        p[node+1][mi]=p[node][mi]\n",
    "        if mi<k-1:\n",
    "            linear_broadcast(node, mi+1, p)\n",
    "            linear_broadcast(node+1, mi, p)\n",
    "    return\n",
    "    \n",
    "linear_broadcast(0, 0, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Pipelined Fibonacci Tree Broadcast\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha     & = \\textrm{Latency}                     \\\\\n",
    "\\beta      & = \\textrm{Communication cost per word} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- Therefore pipelining on binary trees is used, where $m$ is split into an array of $k$ packets of size $\\left\\lceil n/k\\right\\rceil$.\n",
    "- The packets are then broadcast one after another, so that data is distributed fast in the communication network.\n",
    "- Pipelined broadcast on balanced binary tree is possible in $\\mathcal{O}(\\alpha \\log p+\\beta n)$.\n",
    "\n",
    "- <p style=\"color:red\">Shouldn't this be $\\mathcal{O}(\\alpha(\\log p+k))$?!</p>\n",
    "\n",
    "![fig](figures/Fibonacci_Tree_Pipeline.gif)\n",
    "\n",
    "By 13hannes1 - Own work, CC BY-SA 4.0, [curid=77401990](https://commons.wikimedia.org/w/index.php?curid=77401990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fibonacci tree pipeline broadcast with simple recursive algorithm\n",
      "\n",
      "n = 4 k = 2 fibs = [1, 1, 2, 3, 5]\n",
      "LL = 1 LR = 0\n",
      "\n",
      " i:\t [0, 1, 2, 3]\n",
      "di:\t [0, 1, 2, 2]\n",
      "Lt:\t [1, 2, 0, 0]\n",
      "Rt:\t [3, 0, 0, 0]\n",
      "Pt:\t [0, 0, 1, 0]\n",
      "\n",
      "n k c\n",
      "0 0 [[1, 0]]\n",
      "1 0 [[1, 0], [1, 0]]\n",
      "0 1 [[1, 1], [1, 0], [0, 0], [0, 0]]\n",
      "1 1 [[1, 1], [1, 1], [0, 0], [0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print('Fibonacci tree pipeline broadcast with simple recursive algorithm')\n",
    "print()\n",
    "\n",
    "msg  = [1, 1]\n",
    "k    = len(msg)\n",
    "fibs = [1]*(k+3)\n",
    "for i in range (2,k+3):\n",
    "    fibs[i]=fibs[i-1]+fibs[i-2]\n",
    "n = fibs[k+2]-1\n",
    "lastleft  = fibs[k+1]-2\n",
    "lastright = fibs[k]-2\n",
    "print('n =', n, 'k =', k, 'fibs =', fibs)\n",
    "print('LL =', lastleft, 'LR =', lastright)\n",
    "print()\n",
    "\n",
    "index2depth = []\n",
    "for i in range(k+1):\n",
    "    temp = [i]*fibs[i]\n",
    "    index2depth.extend(temp)\n",
    "\n",
    "left   = [i+fibs[index2depth[i]]   for i in range(lastleft+1) ]\n",
    "right  = [i+fibs[index2depth[i]+3] for i in range(lastright+1)] + [0]*(n-lastright-1)\n",
    "parent = [0]*n\n",
    "for i in range(1,len(left)):\n",
    "    parent[left[i]] = i\n",
    "    if right[i] > 0:\n",
    "        parent[right[i]] = i\n",
    "left  += [0]*(n-lastleft-1)\n",
    "\n",
    "print(' i:\\t', [i for i in range(n)])\n",
    "print('di:\\t', index2depth)\n",
    "print('Lt:\\t', left)\n",
    "print('Rt:\\t', right)\n",
    "print('Pt:\\t', parent)\n",
    "print()\n",
    "\n",
    "p     = [[0]*k for i in range(n)]\n",
    "p[0]  = msg\n",
    "ncalls = [[0]*k]\n",
    "\n",
    "print('n', 'k', 'c')\n",
    "def fibonacci_broadcast(node, ki, p, ncalls):\n",
    "    n, k = len(p), len(p[0])\n",
    "    if len(ncalls) - 1 < node:\n",
    "        ncalls.append([0]*k)\n",
    "    if index2depth[node]>k-1 or ki>k-1 or (node > 0 and ncalls[parent[node]][ki] == 0):\n",
    "        return\n",
    "    ncalls[node][ki] += 1\n",
    "    print(node, ki, ncalls)\n",
    "    if left[node]>0:\n",
    "        p[left[node]][ki]  = p[node][ki]\n",
    "        fibonacci_broadcast(left[node], ki, p, ncalls)\n",
    "    if right[node]>0:\n",
    "        p[right[node]][ki] = p[node][ki]\n",
    "        fibonacci_broadcast(right[node], ki, p, ncalls)\n",
    "    if index2depth[node]<k-1:\n",
    "        fibonacci_broadcast(node, ki+1, p, ncalls)\n",
    "    \n",
    "fibonacci_broadcast(0, 0, p, ncalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Broadcast                     | Reduce                     | All-Reduce                    |\n",
    "|-------------------------------|----------------------------|-------------------------------|\n",
    "| ![fig](figures/Broadcast.png) | ![fig](figures/Reduce.png) | ![fig](figures/AllReduce.png) |\n",
    "| [curid=86786881](https://commons.wikimedia.org/w/index.php?curid=86786881)                 |                         [curid=86786880](https://commons.wikimedia.org/w/index.php?curid=86786880)                 |                         [curid=86786874](https://commons.wikimedia.org/w/index.php?curid=86786874)                 |\n",
    "\n",
    "By RenderFlamingo - Own work, CC BY-SA 4.0\n",
    "\n",
    "## 2. Reduce\n",
    "\n",
    "- Reduce is a function the takes an array, combines them using an operator values to give a single value\n",
    "\n",
    "- The reduce pattern is used to collect data or partial results from different processing units and to combine them into a global result by a chosen operator.\n",
    "- Given $p$ processing units, message $m_i$ is on processing unit $p_i$ initially. All $m_i$ are aggregated by $\\otimes$  and the result is eventually stored on $p_0$. The reduction operator $\\otimes$  must be associative at least. Some algorithms require a commutative operator with a neutral element. Operators like $sum$, $min$ and $max$ are common.\n",
    "- Implementation considerations are similar to broadcast. For pipelining on binary trees the message must be representable as a vector of smaller object for component-wise reduction.\n",
    "- Pipelined reduce on a balanced binary tree is possible in ${\\mathcal {O}}(\\alpha \\log p+\\beta n)$.\n",
    "\n",
    "## 3. All-Reduce\n",
    "\n",
    "- The all-reduce pattern (also called allreduce) is used if the result of a reduce operation must be distributed to all processing units. Given $p$ processing units, message $m_i$ is on processing unit $p_i$ initially. All $m_i$ are aggregated by an operator $\\otimes$  and the result is eventually stored on all $p_i$. Analog to the reduce operation, the operator $\\otimes$  must be at least associative.\n",
    "- All-reduce can be interpreted as a reduce operation with a subsequent broadcast. For long messages a corresponding implementation is suitable, whereas for short messages, the latency can be reduced by using a hypercube (Hypercube (communication pattern) § All-Gather/ All-Reduce) topology, if $p$ is a power of two.\n",
    "- All-reduce is possible in ${\\mathcal {O}}(\\alpha \\log p+\\beta n)$, since reduce and broadcast are possible in ${\\mathcal {O}}(\\alpha \\log p+\\beta n)$ with pipelining on balanced binary trees.\n",
    "\n",
    "## 4. Putting it All Together\n",
    "\n",
    "### 4.1. MPI Python Library Function: Bcast\n",
    "\n",
    "```\n",
    "Comm.Bcast( buffer,  # array of size k, i.e. a single message\n",
    "            root=0 ) # data location (root is the processor/node id)\n",
    "```\n",
    "\n",
    "![fig](figures/Bcast.png)\n",
    "\n",
    "### 4.2. MPI Python Library Function \n",
    "\n",
    "```\n",
    "Comm.Reduce( sendbuff,   # send buffer address i.e. word index?\n",
    "             recvbuff,   # recieve buffer address, i.e., only relevant at root=0\n",
    "             op = 'SUM', # reduce operation takes send_buffer\n",
    "             root = 0 )  # data location (root is the processor/node id)\n",
    "```\n",
    "\n",
    "![fig](figures/Red.png)\n",
    "\n",
    "### 4.3. MPI Python Library Function \n",
    "\n",
    "```\n",
    "Comm.Allreduce( sendbuff,    # send buffer address i.e. word index?\n",
    "                recvbuff,    # recieve buffer address, i.e., only relevant at root=0\n",
    "                op = 'SUM' ) # reduce operation takes send_buffer\n",
    "```\n",
    "![fig](figures/AllRed.png)\n",
    "\n",
    "Reduces values on all processes to a single value onto all processes.\n",
    "\n",
    "### 4.4. Communicator class\n",
    "\n",
    "- In mpi4py, ranks are essential to learning about other processes. A rank is the process’s id within a communicator. A process can be part of more than one communicator at any given time.\n",
    "- When `Comm.Get_rank()` is called in your program, it gets called by every process in the communicator variable `comm`, and the rank of each respective process is stored into the variable pointed to by rank.\n",
    "- Remember, rank points to a local variable, which is unique for every calling process because each process has its own separate copy of local variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 4 k = 2 fibs = [1, 1, 2, 3, 5]\n",
      "LL = 1 LR = 0\n",
      "\n",
      " i:\t [0, 1, 2, 3]\n",
      "di:\t [0, 1, 2, 2]\n",
      "Lt:\t [1, 2, 0, 0]\n",
      "Rt:\t [3, 0, 0, 0]\n",
      "Pt:\t [0, 0, 1, 0]\n",
      "\n",
      "i r k ncalls\n",
      "0 1 0 [[1, 0]]\n",
      "1 1 0 [[1, 0], [1, 0]]\n",
      "0 1 1 [[1, 1], [1, 0], [0, 0], [0, 0]]\n",
      "1 1 1 [[1, 1], [1, 1], [0, 0], [0, 0]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, rank, node_size):\n",
    "        self.rank = rank\n",
    "        self.size = node_size # k\n",
    "        self._values = [None]*node_size\n",
    "\n",
    "    def set_value(self, index, value):\n",
    "        if (self._values):\n",
    "            self._values[index] = value\n",
    "        else:\n",
    "            print('Node:', self.index, 'values size must be set')\n",
    "\n",
    "    def get_value(self, index):\n",
    "        return self._values[index]\n",
    "\n",
    "    \n",
    "class Cluster(object):\n",
    "    def __init__(self, num_nodes, node_size):\n",
    "        self._nodes = [Node(i, node_size) for i in range(num_nodes)]\n",
    "        self.size = num_nodes\n",
    "        \n",
    "    def get_node(self, index):\n",
    "        return self._nodes[index]\n",
    "\n",
    "    \n",
    "class Comm(object):\n",
    "    def __init__(self, num_nodes, node_size, bcast_method = 'linear'):\n",
    "        self.ncalls = None\n",
    "        self.fibs = None\n",
    "        self.index2depth = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.parent = None\n",
    "        self.lastleft = None\n",
    "        self.lastright = None\n",
    "        self._check_sizes(num_nodes, node_size, bcast_method)\n",
    "        self.size = num_nodes\n",
    "        self.node_size = node_size\n",
    "        self.bcast_method = bcast_method\n",
    "        self.cluster = Cluster(num_nodes, node_size)\n",
    "        self.rank = 0 # rank is the node that comm is pointing to\n",
    "    \n",
    "    def bcast(self, buffer, root = 0):\n",
    "        \"\"\"Sends buffer to the root process\"\"\"\n",
    "        self.rank = root\n",
    "        if self.bcast_method == 'linear':\n",
    "            for ki in range(self.node_size):\n",
    "                self.ncalls = [0]*self.size\n",
    "                self._binary_bcast(root, root + self.size - 1, ki)\n",
    "        if self.bcast_method == 'fibonacci':\n",
    "            self.ncalls = [[0]*self.node_size]\n",
    "            self._fibonacci_bcast(root, 0, 0)\n",
    "    \n",
    "    def reduce(self, send_ki, recv_ki, op = 'SUM', root = 0):\n",
    "        \"\"\"Reduces values on all processes to a single value onto the root process.\n",
    "        \"\"\"\n",
    "        self.rank = root\n",
    "        temp = [self.cluster.get_node(i).get_value(send_ki) for i in range(self.size)]\n",
    "        result = self.reduce_operation(temp, op)\n",
    "        self.cluster.get_node(root).set_value(recv_ki, result)\n",
    "        return result\n",
    "    \n",
    "    def allreduce(self, send_ki, recv_ki, op = 'SUM'):\n",
    "        \"\"\"Reduces values on all processes to a single value onto all processes.\"\"\"\n",
    "        self.bcast(self.reduce(self, send_ki, recv_ki, op, self.rank), self.rank)\n",
    "    \n",
    "    def reduce_operation(self, values, op = 'SUM'):\n",
    "        if op=='SUM':  return sum(values)\n",
    "        if op=='AVG':  return sum(values) / len(values)\n",
    "\n",
    "    def _check_sizes(self, num_nodes, node_size, bcast_method):\n",
    "        if bcast_method == 'linear':\n",
    "            assert (math.log(num_nodes)/math.log(2)).is_integer()\n",
    "            print('node:', [i for i in range(num_nodes)])\n",
    "            print('i j k ncalls')\n",
    "        if bcast_method == 'fibonacci':\n",
    "            self._fibonacci_numbers(node_size)\n",
    "            assert num_nodes == self.fibs[node_size + 2] - 1\n",
    "            self._setup_fibonacci_tree(num_nodes, node_size)\n",
    "            print('i r k ncalls')\n",
    "    \n",
    "    def _binary_bcast(self, inode, jnode, ki):\n",
    "        \"\"\"Uses divide and conquer to send data to address ki on all processors\"\"\"\n",
    "        if ki == self.node_size - 1:\n",
    "            return\n",
    "        self.ncalls[inode] += 1\n",
    "        word = self.cluster.get_node(inode).get_value(ki)\n",
    "        if jnode - inode == 1:\n",
    "            self.cluster.get_node(jnode%(self.size - 1)).set_value(ki, word)\n",
    "            print(inode, jnode, ki, self.ncalls)\n",
    "            return\n",
    "        else:\n",
    "            newinode = int((inode + jnode) / 2) + 1\n",
    "            self.cluster.get_node(newinode%(self.size - 1)).set_value(ki, word)\n",
    "            print(inode, jnode, ki, self.ncalls)\n",
    "            self._binary_bcast(inode, newinode-1, ki)\n",
    "            self._binary_bcast(newinode, jnode, ki)\n",
    "    \n",
    "    def _fibonacci_numbers(self, node_size):\n",
    "        self.fibs = [1]*(node_size + 3)\n",
    "        for i in range (2, node_size + 3):\n",
    "            self.fibs[i] = self.fibs[i - 1] + self.fibs[i - 2]\n",
    "    \n",
    "    def _setup_fibonacci_tree(self, num_nodes, node_size):\n",
    "        self.lastleft  = self.fibs[node_size + 1] - 2\n",
    "        self.lastright = self.fibs[node_size] - 2\n",
    "        print('n =', num_nodes, 'k =', node_size, 'fibs =', self.fibs)\n",
    "        print('LL =', self.lastleft, 'LR =', self.lastright)\n",
    "        print()\n",
    "\n",
    "        self.index2depth = []\n",
    "        for i in range(node_size + 1):\n",
    "            self.index2depth.extend([i]*self.fibs[i])\n",
    "            \n",
    "        self.left   = [i + self.fibs[self.index2depth[i]    ] for i in range(self.lastleft  + 1)]\n",
    "        self.right  = [i + self.fibs[self.index2depth[i] + 3] for i in range(self.lastright + 1)] \\\n",
    "        + [0]*(num_nodes - self.lastright - 1)\n",
    "        self.parent = [0]*num_nodes\n",
    "        for i in range(1,len(self.left)):\n",
    "            self.parent[self.left[i]] = i\n",
    "            if self.right[i] > 0:\n",
    "                self.parent[self.right[i]] = i\n",
    "        self.left  += [0]*(num_nodes - self.lastleft - 1)\n",
    "\n",
    "        print(' i:\\t', [i for i in range(num_nodes)])\n",
    "        print('di:\\t', self.index2depth)\n",
    "        print('Lt:\\t', self.left)\n",
    "        print('Rt:\\t', self.right)\n",
    "        print('Pt:\\t', self.parent)\n",
    "        print()\n",
    "        \n",
    "    def _fibonacci_bcast(self, root, inode, ki):\n",
    "        if len(self.ncalls) - 1 < inode:\n",
    "            self.ncalls.append([0]*self.node_size)\n",
    "        if self.index2depth[inode] > self.node_size - 1 \\\n",
    "        or ki > self.node_size - 1 \\\n",
    "        or (inode > 0 and self.ncalls[self.parent[inode]][ki] == 0):\n",
    "            return\n",
    "        self.ncalls[inode][ki] += 1\n",
    "        word = self.cluster.get_node(root).get_value(ki)\n",
    "        print(inode, root, ki, self.ncalls)\n",
    "        if self.left[inode] > 0:\n",
    "            self.cluster.get_node((self.left[inode] + root) % self.size).set_value(ki, word)\n",
    "            self._fibonacci_bcast(root, self.left[inode], ki)\n",
    "        if self.right[inode] > 0:\n",
    "            self.cluster.get_node((self.right[inode] + root) % self.size).set_value(ki, word)\n",
    "            self._fibonacci_bcast(root, self.right[inode], ki)\n",
    "        if self.index2depth[inode] < self.node_size - 1:\n",
    "            self._fibonacci_bcast(root, inode, ki+1)\n",
    "\n",
    "n    = 4\n",
    "msg  = [1,1]\n",
    "k = len(msg)\n",
    "comm = Comm(n, k, 'fibonacci')\n",
    "# Put the message at node\n",
    "start_node = 1\n",
    "for i, m in enumerate(msg):\n",
    "    comm.cluster.get_node(start_node).set_value(i, m)\n",
    "comm.bcast(comm.cluster.get_node(start_node), start_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Examples\n",
    "\n",
    "### 5.1. Calculate the Mean Using Reduce\n",
    "\n",
    "- Each process creates random numbers and makes a local_sum calculation.\n",
    "- The local_sum is then reduced to the root process using MPI_SUM.\n",
    "- The global average is then global_sum / (world_size * num_elements_per_proc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  \t 0.5319006758610212\n",
      "stddev:\t 0.5964855071963291\n"
     ]
    }
   ],
   "source": [
    "random.seed(6)\n",
    "values = [[random.random() for i in range(k)] for j in range(n)]\n",
    "\n",
    "lsums = [sum(values[i]) for i in range(len(values))]\n",
    "\n",
    "gsum = sum(lsums)\n",
    "gavg = gsum / (n*k)\n",
    "\n",
    "lsumsq = [sum([values[i][j]*values[i][j] for j in range(k)]) for i in range(n)]\n",
    "gsumsq = sum(lsumsq)\n",
    "\n",
    "gstd = math.sqrt(gsumsq / (n*k))\n",
    "\n",
    "print('mean:  \\t', gavg)\n",
    "print('stddev:\\t', gstd)\n",
    "\n",
    "for i in range(n):\n",
    "    node = comm.cluster.get_node(i).set_value(j, lsums[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmean: 0.5319006758610212\n"
     ]
    }
   ],
   "source": [
    "for i in range(n):\n",
    "    node_sum = comm.reduce_operation(comm.cluster.get_node(i)._values, 'SUM')\n",
    "    comm.cluster.get_node(i).set_value(0, node_sum)\n",
    "print('rmean:', comm.reduce(0,0,'SUM',0)/(n*k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Calculate the Standard Deviation Using AllReduce\n",
    "\n",
    "- Each process computes the local_sum of elements and sums them using MPI_Allreduce.\n",
    "- After the global sum is available on all processes, the mean is computed so that local_sq_diff can be computed.\n",
    "- Once all of the local squared differences are computed, global_sq_diff is found by using MPI_Reduce.\n",
    "- The root process can then compute the standard deviation by taking the square root of the mean of the global squared differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
